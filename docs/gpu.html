<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>Effort Engine</title>
    <link rel="stylesheet" href="tufte.css"/>
    <link rel="stylesheet" href="latex.css"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  </head>

  <body>
  <div style="float: right; padding-right:25px; margin-right:0px; line-height:1.5; margin-top: 10px">
  <a href="index.html">Home</a><br>

  <a href="equations.html">The Basics</a><br>
   <a href="bucketmul.html">Introducing bucketMul</a>
   <br>
      <a href="gpu.html">The GPU implementation</a><br>
    <a href="q8.html">MoE</a><br>
     <a href="pesky.html">Pesky details</a><br>
     <a href="about.html">About the Author(s)</a> <br>
     <a href="setup.html">Download and Run</a>
</div>

    <article>

      <h1 id="tufte-css">Implementing bucketMul</h1>
      <p class="subtitle">in Metal</p>
      <section>

<p>
Below is a simplified version of the main <b>bucketMul</b> function. This function takes dispatch as a parameter, along with model weights, and outputs multiplications into the result.

<pre>
kernel void bucketMul(
                   device const half *<b>weights</b> [[buffer(0)]],
                   device const float2 *<b>dispatch</b> [[buffer(1)]],
                   device float *<b>result</b> [[buffer(2)]],
                   constant uint *<b>dispatchSize</b> [[buffer(3)]],
                   uint <b>bucketID</b> [[thread_position_in_grid]]) {
                      
    float myVal[16] = {0};
      
    for (uint r=0; r&lt;dispatchSize; r+=1) {
            float2 d = dispatch[rowOffset]; // d[0] is weight, d[1] is rowId
            half w = weights[int(d[1]) + bucketID]; // Get weight based on dispatch

            float v = d[0]*float(w); // Perform multiplication
            ushort pos = as_type&lt;ushort>(w) & 15; // Get position for the result
            
            for (int i=0; i&lt;16; i++) {
                myVal[i] += (pos == i) ? v : 0; 
            }
    }

    // Store results in the output buffer
    for (int i = 0; i&lt;16; i++) {
        result[myOff + i] = myVal[i];
    }
                          
}
</pre>

<p>
Readers new to GPU programming might wonder, <b>'How does it work?'</b>?
<p>
Readers experienced with GPU programming might ask, <b>'How on earth does it work?'</b>

<h2>Basics of Metal</h2>
<p>
The above code is what's known as a shader function. It is invoked from Swift with a line like this:
<p><span class="marginnote">
wrapper to actual Swift functions - makes for easier reading
</span>

<pre>

gpu.deploy("bucketMulFast",
           buffers:     [ weightBuckets,
                          dispatch,
                          tmpMulVec,
                          dispatch.size],
           threadCount: [ weightBuckets.cols ])
</pre>
<p>
This function is invoked <b>`threadCount`</b> times, each time receiving the same parameters. The only difference is the bucketID, which represents each thread's position in the gridâ€”defined here as <b>[weightBuckets.cols]</b>.
<p>
Each call gets the same parameters, with the only difference being, in this case - bucketID, which is a thread's position in the grid. Grid in this case is simply [ weightBuckets.cols ].
<p>
Each call performs the computations, and writes out the result into the result buffer.
<p>
Crucially, Apple's GPUs organize threads into SIMD-groups of 32, simplyfing <i>wildly</i>.
<p>
SIMD (Same Instruction, Multiple Data) groups execute all operations in lockstep, performing identical operations but potentially on different data.
<p>
There is no need to synchronize the threads within a single group, as they always operate in unison. If one thread encounters an 'if' condition, the others will wait for this condition to resolve before proceeding.
<p>
That's it for the basics. If you did any programming in your life, the code should be quite simple to understand:
<p>
- it fetches a row from dispatch: each row has the value, and an id of a bucket row - we'll be multiplying the value by the buckets in the bucket row
<p>
- for every bucketID, it grabs it's weight, it grabs the position from the least significant bits (& 15), and it multiplies the rest by the value, putting it into local memory (which is super-fast to access)
<p>
- finally, it outputs the result into the device memory

<h2>Why BucketMul works fast</h2>
<p>
<span class="marginnote">
            This whole section needs rewriting and feedback. Please <a href="mailto:kolinko@gmail.com">reach out</a> if you know anything about Metal programming.
          </span>

  Now, this may be obvious to some, but it wasn't obvious for me. Apple's documentation is lacking, disassembly not easily accessible, and  I believe I may have bent some rules while crafting the code shown above.

<p>
A better explaination will be most welcome.
<p>
Here are my theories on why such straightforward code is effective:
<p>
- <b>Dispatch Load</b>: The data involved is minimal and gets cached quickly, reducing the need for extensive coordination during loading.
<p>
- <b>Weights Load</b>: Reads are implicitly synchronized, with SIMD-group threads loading adjacent elements, allowing high-speed, consecutive reads. We're going against every known manual that says the reads should be consecutive within a single thread, not across threads - but somehow it seems to work?

<p>

  <pre>
- <b>myVal[i] loop</b> An unrolled micro-loop uses cheap three-way operators, minimizing random memory access. A simpler approach like 'myVal[j] += v' didn't work if I checked correctly.

- <b>myVal storage and speed:</b>
   - this is the biggest mystery to me!
   - it seems it's the main bottleneck in calculations
   - increasing size of myVal (say to 32) lowers the speed of operation twice
   - keeping the size intact, but using just a fraction of the values speeds up the operation twice (!) - if you change & 15 to & 7, or modify underlying data to be in a smaller range (!!!), you will get a performance boost
   - where is myVal stored and at what form? it won't fit the registers, it doesn't go to device memory, so I guess some sort of an intermediate cache? If anyone can shine light at it, I will appreciate it
</pre>

<p>
The storage of myVal may also be a reason why bucketMool seems to have a higher speed on M3 compared to M2/M1. M3 has their cache/registry structure reorganised.
<h2>
Why buckets are sized 16.
</h2>
<p>
If we switched from 16 to 32, we lose an extra bit for storing weights (can be avoided - see next chapter), and above all - <b>the speed gets 2x lower</b>, so we can't go that way.


  <div class="epigraph">
          <blockquote>
            <p>
The higher the bucket size, the more precise the bucket sorting (see previous chapter). So ideally, we'd like as large of a bucket as possible.
</p></blockquote></div>
<p>
If we go into a lower bucket size, say 8, the memory ordering won't be as precise, and we'll need higher effort for the same result. With Q8 that's what we needed to do though. Also, if I remember correctly - with buckets sized 8 - execution slowed down in other places and we ended up in the same place as before - speed-wise.

<h2>
Overhead performance challenges</h2>

This is my first program that tackles GPU development, and I could seriously use help here.
<p>
  While the bucketMul is implemented fast enough - reaching parity with Apple's MPS at 50-70% effort, the rest of the inference process has an overhead that I have no idea how to fix. It definitely is fixable, since Llama.cpp and others don't seem to have it.
<p>
  In the GPU profiler, I can see holes between certain kernel invocations, and I cannot figure out where they come from.
</p>
<figure>
  <img src="ryc/ryc3.1.png">
</figure>
<p>
I think these holes sum up to 15ms overhead to every token generation. In my case, even with multiplications set to zero, I can generate max ~60tps on my laptop. If you have an idea on how to fix that, please I beg shoot me an e-mail: kolinko@gmail.com.
<p>
It has probably something to do with the way I call commandQueue etc, or because of the buffers are hazard-tracked. But I spent days trying to figure it out, and just cannot find the reason. The relevant code is in <a href="https://github.com/kolinko/effort/blob/b4257f460ea527f8accac3a809de1e9d7ad79541/runNetwork.swift#L176">here</a>, <a href="https://github.com/kolinko/effort/blob/b4257f460ea527f8accac3a809de1e9d7ad79541/bucketMul.swift#L14"> and <a href="https://github.com/kolinko/effort/blob/b4257f460ea527f8accac3a809de1e9d7ad79541/helpers/gpu.swift#L146">here</a>. 
<p>
Without this, we have a Ferrari engine in a Fiat body here - no simple way to show the results and do serious benchmarking.
<p>
  <p>
<h3>Quality issues even at 100% effort</h3>
<p><span class="marginnote">
May actually work fine. I began benchmarking KL distance now, and it seems at effort 100% it's giving the same results as the original. Will post an update soon. -- TK 21.04.2024
</span>
The second challenge is that the current implementation seems just broken. It used to deliver results that were very close to the original Mistral, and Mixtral used to work. Right now Mistral is not as good as it used to be, and Mixtral barely works. I'm at my wits' end trying to debug it all - if anyone knows enough about both GPU debugging and LLM implementations, I would appreciate any help here.
<h2>Could bucketMul run even faster?</h2>
<p>
Could we improve bucketMul to get to full 100% memory bandwidth read speed? Right now it's around 50-70%. I tried simdgroup-coordinated reads and other tricks (you can see them in the helpers/wild sources), but failed to write anything better than this.
</p>
<figure>
<img src="ryc/ryc3.2.png">
Half the max practical memory read speed. Limited by ALU. It's still amazing, but there may be room for improvement.
</figure>
<p>
  Unlike the overhead delays, this is not a priority.
<p>
  <h3>Optimisation for different matrix sizes</h3>
  <p>
The current implementation is optimised for a matrix shape of 4096x14336 (w1 and w3). It can be just as fast on 14336x4096 and 4096x4096 , but needs hand-tuning the number of threads/groups for every case. An easy fix, just needs some engineering time.
<p>
<h3>Dispatch non-determinism</h3>
<p>
It's a minor issue, but difficult to fix. Dispatch uses atomic loats to generate ids. It doesn't affect speed much, but it causes the whole token generation to be nondeterministic. It is a bit irritating during testing, but the overall effect is just a slight unpredictibality of the output. Fixing this is a low priority.


<h2>Where do we go from here?</h2>
    <p>
    - <a href="q8.html">MoE, Quantization and the others.</a>
    </p>
    <p>
      - <a href="pesky.html">Pesky details (or: Help Needed!)</a>
    </p>
    <h2>
      At any time
    <p>
      - <a href="setup.html">Install and Download</a>
    </p>
    <h2>And of course...</h2>
    <p>
      - <a href="about.html">About the Author(s)</a>
    </p>
    <p>
      - Citations, notes and so on
    </p>
    <h2>Or going back...</h2>
    <p>
      - <a href="index.html">The landing page.</a>
    </p>
    <p>
      - <a href="equations.html">The basics.</a>
    </p>
  <p>
    - <a href="bucketmul.html">Introducing bucketMul.</a>
  </p>

    
      </section>
    </article>
  </body>
</html>

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>Effort Engine</title>
    <link rel="stylesheet" href="tufte.css"/>
    <link rel="stylesheet" href="latex.css"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  </head>

  <body>
  <div style="float: right; padding-right:25px; margin-right:0px; line-height:1.5; margin-top: 10px">
  <a href="index.html">Home</a><br>

  <a href="equations.html">The Basics</a><br>
   <a href="bucketmul.html">Introducing bucketMul</a>
   <br>
      <a href="gpu.html">The GPU implementation</a><br>
    <a href="q8.html">MoE</a><br>
     <a href="pesky.html">Pesky details</a><br>
     <a href="about.html">About the Author(s)</a> <br>
     <a href="setup.html">Download and Run</a>
</div>

    <article>

      <h1 id="tufte-css">Implementing bucketMul</h1>
      <p class="subtitle">in Metal</p>
      <section>

<p>
Below is a simplified version of the main <b>bucketMul</b> function. This function takes dispatch as a parameter, along with model weights, and outputs multiplications into the result.

<pre>
kernel void bucketMul(
                   device const half *<b>weights</b> [[buffer(0)]],
                   device const float2 *<b>dispatch</b> [[buffer(1)]],
                   device float *<b>result</b> [[buffer(2)]],
                   constant uint *<b>dispatchSize</b> [[buffer(3)]],
                   uint <b>bucketID</b> [[thread_position_in_grid]]) {
                      
    float myVal[16] = {0};
      
    for (uint r=0; r&lt;dispatchSize; r+=1) {
            float2 d = dispatch[rowOffset]; // d[0] is weight, d[1] is rowId
            half w = weights[int(d[1]) + bucketID]; // Get weight based on dispatch

            float v = d[0]*float(w); // Perform multiplication
            ushort pos = as_type&lt;ushort>(w) & 15; // Get position for the result
            
            for (int i=0; i&lt;16; i++) {
                myVal[i] += (pos == i) ? v : 0; 
            }
    }

    // Store results in the output buffer
    for (int i = 0; i&lt;16; i++) {
        result[myOff + i] = myVal[i];
    }
                          
}
</pre>

<p>
Readers new to GPU programming might wonder, <b>'How does it work?'</b>?
<p>
Readers experienced with GPU programming might ask, <b>'How on earth does it work?'</b>

<h2>Basics of Metal</h2>
<p>
The above code is what's known as a shader function. It is invoked from Swift with a line like this:
<p><span class="marginnote">
wrapper to actual Swift functions - makes for easier reading
</span>

<pre>

gpu.deploy("bucketMulFast",
           buffers:     [ weightBuckets,
                          dispatch,
                          tmpMulVec,
                          dispatch.size],
           threadCount: [ weightBuckets.cols ])
</pre>
<p>
This function is invoked <b>`threadCount`</b> times, each time receiving the same parameters. The only difference is the bucketID, which represents each thread's position in the gridâ€”defined here as <b>[weightBuckets.cols]</b>.
<p>
Each call gets the same parameters, with the only difference being, in this case - bucketID, which is a thread's position in the grid. Grid in this case is simply [ weightBuckets.cols ].
<p>
Each call performs the computations, and writes out the result into the result buffer.
<p>
Crucially, Apple's GPUs organize threads into SIMD-groups of 32, simplyfing <i>wildly</i>.
<p>
SIMD (Same Instruction, Multiple Data) groups execute all operations in lockstep, performing identical operations but potentially on different data.
<p>
There is no need to synchronize the threads within a single group, as they always operate in unison. If one thread encounters an 'if' condition, the others will wait for this condition to resolve before proceeding.
<p>
That's it for the basics. If you did any programming in your life, the code should be quite simple to understand:
<p>
- it fetches a row from dispatch: each row has the value, and an id of a bucket row - we'll be multiplying the value by the buckets in the bucket row
<p>
- for every bucketID, it grabs it's weight, it grabs the position from the least significant bits (& 15), and it multiplies the rest by the value, putting it into local memory (which is super-fast to access)
<p>
- finally, it outputs the result into the device memory

<h2>Why BucketMul works fast</h2>
<p>
<span class="marginnote">
            This whole section needs rewriting and feedback. Please <a href="mailto:kolinko@gmail.com">reach out</a> if you know anything about Metal programming.
          </span>

  Now, this may be obvious to some, but it wasn't obvious for me. Apple's documentation is lacking, disassembly not easily accessible, and I think I broke some rules while writing the code above. 

<p>
A better explaination will be most welcome.
<p>
Here are my theories on why such straightforward code is effective:
<p>
- <b>Dispatch Load</b>: The data involved is minimal and gets cached quickly, reducing the need for extensive coordination during loading.
<p>
- <b>Weights load</b> - since we have implicitly coordinated reads (threads from each simdgroup load elements that are next to each other), the reads get treated as consecutive, and we can perform them at a high speed. We're going against every known manual that says the reads should be consecutive within a single thread, not across threads - but somehow it seems to work?

<p>

  <pre>
- myVal[i] loop:
   - we have a micro-loop that gets unrolled
   - three-way operator is very cheap in Metal
   - we avoid random memory access this way
   - myVal[j] += v might also work, but sometimes I had issues with performance, I think

- myVal storage and speed:
   - this is the biggest mystery to me!
   - it seems it's the main slowdown of the kernel
   - increasing size of myVal (say to 32) lowers the speed of operation twice
   - keeping the size intact, but using just a fraction of the values speeds up the operation twice (!) - if you change & 15 to & 7, or modify underlying data to be in a smaller range (!!!), you will get a performance boost
   - where is myVal stored and at what form? it won't fit the registers, it doesn't go to device memory, so I guess some sort of an intermediate cache? If anyone can shine light at it, I will appreciate it
</pre>

<p>
The storage of myVal may also be a reason why bucketMool seems to have a higher speed on M3 compared to M2/M1. M3 has their cache/registry structure reorganised.
<h2>
Why buckets are sized 16.
</h2>
<p>
If we switched from 16 to 32, we lose an extra bit for storing weights (can be avoided - see next chapter), and above all - <b>the speed gets 2x lower</b>, so we can't go that way.


  <div class="epigraph">
          <blockquote>
            <p>
The higher the bucket size, the more precise the bucket sorting (see previous chapter). So ideally, we'd like as large of a bucket as possible.
</p></blockquote></div>
<p>
If we go into a lower bucket size, say 8, the memory ordering won't be as precise, and we'll need higher effort for the same result. With Q8 that's what we needed to do though. Also, if I remember correctly - with buckets sized 8 - execution slowed down in other places and we ended up in the same place as before - speed-wise.

<h2>
Other implementation details.</h2>

This is my first program that tackles GPU development, and I could seriously use help here.

<p>
Most importantly: where does the overhead of overall calculations come from and how to get rid of it?

(performance screenshot here)

I know llama.cpp somehow doesn't have that overhead - judging by their numbers. In my case, even with multiplications set to zero, I keep having the delays between kernel invocations. If you have an idea on how to fix that, please I beg shoot me an e-mail: kolinko@gmail.com.

Without this, we have a Ferrari engine in a Fiat body here - no simple way to show the results here.

The second challenge is that the current implementation seems just broken. It used to deliver results that were very close to the original Mistral, and Mixtral used to work. Right now Mistral is not as good as it used to be, and Mixtral barely works. I'm at my wits' end trying to debug it all - if anyone knows enough about both GPU debugging and LLM implementations, I would appreciate any help here.

Another case entirely is of couse the BucketMul implementation itself - could it be improved to get to full 100% memory bandwidth read? I tried simdgroup-coordinated reads and other tricks (you can see them in helpers/wild), but failed to write anything better than this.

And finally - probably the easiest, but I just didn't have the time - right now the hyperparameters (num steps, groups) of multiplications are optimised for matrixes of a certain shape (w1 & w3 of Mistral). Other sizes can work as fast - I've seen it in some experiments, but I'd love to see a robust implementation here scaling to all sizes.

Dispatch uses atomic floats to generate ids - speed aside, it causes the whole run to be nondeterministic - dispatch ends up in a different order every single time, and float adds in bucketmul are executed in a different order, resulting in a small dose of non-determinism. On the one hand it's nice, because we end up with a natural temperature to the model, but it makes testing difficult. Ideas here would be welcome.


<h2>Where do we go from here?</h2>
    <p>
    - <a href="q8.html">MoE, Quantization and the others.</a>
    </p>
    <p>
      - <a href="pesky.html">Pesky details (or: Help Needed!)</a>
    </p>
    <h2>
      At any time
    <p>
      - <a href="setup.html">Install and Download</a>
    </p>
    <h2>And of course...</h2>
    <p>
      - <a href="about.html">About the Author(s)</a>
    </p>
    <p>
      - Citations, notes and so on
    </p>
    <h2>Or going back...</h2>
    <p>
      - <a href="index.html">The landing page.</a>
    </p>
    <p>
      - <a href="equations.html">The basics.</a>
    </p>
  <p>
    - <a href="bucketmul.html">Introducing bucketMul.</a>
  </p>

    
      </section>
    </article>
  </body>
</html>
